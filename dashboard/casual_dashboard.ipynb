{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc7d837-aaa0-4834-98da-877bab170639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:00:13.777 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-25 20:00:13.785 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "# ------------------ PAGE CONFIG ------------------\n",
    "st.set_page_config(page_title=\"Causal Inference Analysis\", layout=\"wide\")\n",
    "\n",
    "\n",
    "# ------------------ LOAD DATA ------------------\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    try:\n",
    "        data = pd.read_csv(\"sml_project/datasets/balanced_ihdata.csv\")\n",
    "        data = data.loc[:, ~data.columns.str.contains(\"^Unnamed\")]  # remove unnamed cols\n",
    "        data[\"treatment\"] = data[\"treatment\"].astype(float)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        st.warning(\"balanced_ihdata.csv not found. Using synthetic data instead.\")\n",
    "        return create_synthetic_data()\n",
    "\n",
    "\n",
    "def create_synthetic_data(n=500, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    data = pd.DataFrame({\n",
    "        \"treatment\": np.concatenate([np.ones(n//2), np.zeros(n//2)]),\n",
    "        \"income\": np.random.normal(50000, 10000, n),\n",
    "        \"birth_weight\": np.random.normal(3000, 500, n),\n",
    "        \"parent_edu\": np.random.randint(8, 18, n),\n",
    "        \"health_index\": np.random.normal(70, 15, n),\n",
    "        \"housing_quality\": np.random.normal(6, 2, n),\n",
    "        \"neighborhood_safety\": np.random.normal(7, 2, n)\n",
    "    })\n",
    "    beta = np.array([0.5, 0.3, 0.8, 0.4, 0.6, 0.2])\n",
    "    confounders = [\"income\", \"birth_weight\", \"parent_edu\", \"health_index\", \"housing_quality\", \"neighborhood_safety\"]\n",
    "    X = data[confounders].values\n",
    "    data[\"mu0\"] = X.dot(beta) + np.random.normal(0, 1, n)\n",
    "    data[\"mu1\"] = data[\"mu0\"] + 5 + 0.1 * data[\"income\"]/10000 + 0.2 * data[\"parent_edu\"]\n",
    "    data[\"outcome_factual\"] = np.where(data[\"treatment\"] == 1, data[\"mu1\"], data[\"mu0\"])\n",
    "    data[\"outcome_counterfactual\"] = np.where(data[\"treatment\"] == 1, data[\"mu0\"], data[\"mu1\"])\n",
    "    for i in range(11, 26):\n",
    "        data[f\"x{i}\"] = np.random.normal(0, 1, n)\n",
    "    return data\n",
    "\n",
    "\n",
    "if 'data' not in st.session_state or st.sidebar.button(\"Reset Data\"):\n",
    "    st.session_state.data = load_data()\n",
    "    st.session_state.models_fitted = False\n",
    "\n",
    "data = st.session_state.data\n",
    "confounders = [\"income\", \"birth_weight\", \"parent_edu\", \"health_index\", \"housing_quality\", \"neighborhood_safety\"]\n",
    "\n",
    "if 'scaler' not in st.session_state:\n",
    "    st.session_state.scaler = StandardScaler().fit(data[confounders])\n",
    "\n",
    "\n",
    "# ------------------ MODEL TRAINING ------------------\n",
    "@st.cache_data(show_spinner=True)\n",
    "def train_model(data, treatment_group, confounders):\n",
    "    subset = data[data[\"treatment\"] == treatment_group]\n",
    "    if len(subset) == 0:\n",
    "        raise ValueError(f\"No data available for treatment group {treatment_group}\")\n",
    "\n",
    "    X, y = subset[confounders], subset[\"outcome_factual\"]\n",
    "    X_scaled = st.session_state.scaler.transform(X)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    return model, r2, mse\n",
    "\n",
    "\n",
    "def predict_outcome(model, input_data):\n",
    "    try:\n",
    "        input_data_scaled = st.session_state.scaler.transform(input_data)\n",
    "        return model.predict(input_data_scaled)[0]\n",
    "    except NotFittedError:\n",
    "        st.error(\"Model not fitted yet.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ------------------ ATE CALCULATION ------------------\n",
    "def calculate_ate(data):\n",
    "    return data[\"mu1\"].mean() - data[\"mu0\"].mean()\n",
    "\n",
    "\n",
    "def propensity_score_matching(data, confounders):\n",
    "    X = data[confounders]\n",
    "    y = data['treatment']\n",
    "    propensity_model = LogisticRegression(max_iter=1000)\n",
    "    propensity_model.fit(X, y)\n",
    "    propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    treated = data[data['treatment'] == 1]\n",
    "    control = data[data['treatment'] == 0]\n",
    "\n",
    "    matched_outcomes, matched_ites = [], []\n",
    "\n",
    "    for _, treated_row in treated.iterrows():\n",
    "        x_treated = pd.DataFrame([treated_row[confounders].values], columns=confounders)\n",
    "        treated_ps = propensity_model.predict_proba(x_treated)[:, 1]\n",
    "\n",
    "        control_ps = propensity_model.predict_proba(control[confounders])[:, 1]\n",
    "        distances = np.abs(control_ps - treated_ps)\n",
    "        closest_control_idx = distances.argmin()\n",
    "\n",
    "        treated_outcome = treated_row['outcome_factual']\n",
    "        control_outcome = control.iloc[closest_control_idx]['outcome_factual']\n",
    "        ite = treated_outcome - control_outcome\n",
    "\n",
    "        matched_outcomes.append((treated_outcome, control_outcome))\n",
    "        matched_ites.append(ite)\n",
    "\n",
    "    return {\n",
    "        'matched_ate': np.mean(matched_ites),\n",
    "        'matched_outcomes': matched_outcomes,\n",
    "        'pscore_balance': propensity_model.coef_[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def doubly_robust_estimation(data, confounders):\n",
    "    propensity_model = LogisticRegression(max_iter=1000)\n",
    "    propensity_model.fit(data[confounders], data['treatment'])\n",
    "    propensity_scores = propensity_model.predict_proba(data[confounders])[:, 1]\n",
    "\n",
    "    outcome_model_treated = LinearRegression()\n",
    "    outcome_model_control = LinearRegression()\n",
    "\n",
    "    outcome_model_treated.fit(\n",
    "        data[data['treatment'] == 1][confounders], \n",
    "        data[data['treatment'] == 1]['outcome_factual']\n",
    "    )\n",
    "    outcome_model_control.fit(\n",
    "        data[data['treatment'] == 0][confounders], \n",
    "        data[data['treatment'] == 0]['outcome_factual']\n",
    "    )\n",
    "\n",
    "    dr_estimates = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        # always keep DataFrame with correct column names\n",
    "        row = data.loc[i, confounders]\n",
    "        x = pd.DataFrame([row.values], columns=confounders)\n",
    "\n",
    "        true_treatment = data.loc[i, 'treatment']\n",
    "        outcome_treated = outcome_model_treated.predict(x)[0]\n",
    "        outcome_control = outcome_model_control.predict(x)[0]\n",
    "        ps = propensity_scores[i]\n",
    "\n",
    "        dr_estimate = (\n",
    "            true_treatment * (data.loc[i, 'outcome_factual'] - outcome_treated) / ps +\n",
    "            (1 - true_treatment) * (data.loc[i, 'outcome_factual'] - outcome_control) / (1 - ps) +\n",
    "            outcome_treated - outcome_control\n",
    "        )\n",
    "        dr_estimates.append(dr_estimate)\n",
    "\n",
    "    return {\n",
    "        'doubly_robust_ate': np.mean(dr_estimates),\n",
    "        'dr_estimates': dr_estimates\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------ STREAMLIT APP ------------------\n",
    "page = st.sidebar.radio(\"Navigate\", [\"Input & Results\", \"Dataset Overview\", \"Model Diagnostics\"])\n",
    "\n",
    "n_treated = (data[\"treatment\"] == 1).sum()\n",
    "n_control = (data[\"treatment\"] == 0).sum()\n",
    "st.sidebar.write(f\"Dataset: {len(data)} samples\")\n",
    "st.sidebar.write(f\"- Treated group: {n_treated}\")\n",
    "st.sidebar.write(f\"- Control group: {n_control}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    if 'models_fitted' not in st.session_state or not st.session_state.models_fitted:\n",
    "        with st.spinner(\"Training models...\"):\n",
    "            st.session_state.treated_model, st.session_state.treated_r2, st.session_state.treated_mse = train_model(data, 1, confounders)\n",
    "            st.session_state.control_model, st.session_state.control_r2, st.session_state.control_mse = train_model(data, 0, confounders)\n",
    "            st.session_state.models_fitted = True\n",
    "except Exception as e:\n",
    "    st.error(f\"Error training models: {str(e)}\")\n",
    "    st.session_state.models_fitted = False\n",
    "\n",
    "\n",
    "if page == \"Input & Results\":\n",
    "    st.title(\"Causal Inference Analysis\")\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        income = st.slider(\"Income\", float(data[\"income\"].min()), float(data[\"income\"].max()), float(data[\"income\"].median()))\n",
    "        birth_weight = st.slider(\"Birth Weight\", float(data[\"birth_weight\"].min()), float(data[\"birth_weight\"].max()), float(data[\"birth_weight\"].median()))\n",
    "        parent_edu = st.slider(\"Parent Education\", int(data[\"parent_edu\"].min()), int(data[\"parent_edu\"].max()), int(data[\"parent_edu\"].median()))\n",
    "    with col2:\n",
    "        health_index = st.slider(\"Health Index\", float(data[\"health_index\"].min()), float(data[\"health_index\"].max()), float(data[\"health_index\"].median()))\n",
    "        housing_quality = st.slider(\"Housing Quality\", float(data[\"housing_quality\"].min()), float(data[\"housing_quality\"].max()), float(data[\"housing_quality\"].median()))\n",
    "        neighborhood_safety = st.slider(\"Neighborhood Safety\", float(data[\"neighborhood_safety\"].min()), float(data[\"neighborhood_safety\"].max()), float(data[\"neighborhood_safety\"].median()))\n",
    "\n",
    "    treatment = st.radio(\"Treatment Applied?\", [\"Yes\", \"No\"])\n",
    "    treatment_value = 1 if treatment == \"Yes\" else 0\n",
    "\n",
    "    if st.session_state.models_fitted:\n",
    "        input_data = pd.DataFrame({\n",
    "            \"income\": [income],\n",
    "            \"birth_weight\": [birth_weight],\n",
    "            \"parent_edu\": [parent_edu],\n",
    "            \"health_index\": [health_index],\n",
    "            \"housing_quality\": [housing_quality],\n",
    "            \"neighborhood_safety\": [neighborhood_safety]\n",
    "        })\n",
    "\n",
    "        treated_outcome = predict_outcome(st.session_state.treated_model, input_data)\n",
    "        control_outcome = predict_outcome(st.session_state.control_model, input_data)\n",
    "\n",
    "        if treated_outcome is not None and control_outcome is not None:\n",
    "            individual_treatment_effect = treated_outcome - control_outcome\n",
    "            factual_outcome = treated_outcome if treatment_value else control_outcome\n",
    "            counterfactual_outcome = control_outcome if treatment_value else treated_outcome\n",
    "\n",
    "            psm_results = propensity_score_matching(data, confounders)\n",
    "            dre_results = doubly_robust_estimation(data, confounders)\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.metric(\"Population ATE\", f\"{calculate_ate(data):.4f}\")\n",
    "                st.metric(\"Your Factual Outcome\", f\"{factual_outcome:.4f}\")\n",
    "                st.metric(\"PSM ATE\", f\"{psm_results['matched_ate']:.4f}\")\n",
    "            with col2:\n",
    "                st.metric(\"Counterfactual Outcome\", f\"{counterfactual_outcome:.4f}\")\n",
    "                st.metric(\"ITE\", f\"{individual_treatment_effect:.4f}\")\n",
    "                st.metric(\"Doubly Robust ATE\", f\"{dre_results['doubly_robust_ate']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f30dd7-9e1f-4ba8-b74e-c6b1881659e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
